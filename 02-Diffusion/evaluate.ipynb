{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoos-bii/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms as T, utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import os\n",
    "\n",
    "from denoising_diffusion_pytorch.fid_evaluation import FIDEvaluation\n",
    "\n",
    "from classifier_free_guidance import Unet, GaussianDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(image_size, folder, batch_size):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_size),  # args.image_size + 1/4 *args.image_size\n",
    "        torchvision.transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = torchvision.datasets.ImageFolder(folder, transform=transforms)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    split_batches = True,\n",
    "    mixed_precision = 'fp16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f91480ccaf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = get_data(256, '/home/yoos-bii/Desktop/data_tct/val', 1)\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.9451, 0.9490, 0.9412,  ..., 0.6549, 0.6471, 0.6863],\n",
       "           [0.9373, 0.9412, 0.9451,  ..., 0.6588, 0.6392, 0.6627],\n",
       "           [0.9412, 0.9373, 0.9451,  ..., 0.7020, 0.6863, 0.6902],\n",
       "           ...,\n",
       "           [0.9412, 0.9412, 0.9451,  ..., 0.9529, 0.9608, 0.9647],\n",
       "           [0.9451, 0.9451, 0.9412,  ..., 0.9569, 0.9490, 0.9451],\n",
       "           [0.9412, 0.9451, 0.9490,  ..., 0.9569, 0.9490, 0.9451]],\n",
       " \n",
       "          [[0.9216, 0.9255, 0.9176,  ..., 0.7961, 0.7804, 0.8157],\n",
       "           [0.9137, 0.9176, 0.9216,  ..., 0.7922, 0.7686, 0.7882],\n",
       "           [0.9176, 0.9137, 0.9216,  ..., 0.8314, 0.8118, 0.8118],\n",
       "           ...,\n",
       "           [0.9176, 0.9176, 0.9216,  ..., 0.9294, 0.9373, 0.9412],\n",
       "           [0.9216, 0.9216, 0.9176,  ..., 0.9333, 0.9255, 0.9216],\n",
       "           [0.9176, 0.9216, 0.9255,  ..., 0.9333, 0.9255, 0.9216]],\n",
       " \n",
       "          [[0.9373, 0.9412, 0.9333,  ..., 0.8471, 0.8314, 0.8588],\n",
       "           [0.9294, 0.9333, 0.9373,  ..., 0.8510, 0.8196, 0.8314],\n",
       "           [0.9333, 0.9294, 0.9373,  ..., 0.8902, 0.8627, 0.8549],\n",
       "           ...,\n",
       "           [0.9294, 0.9294, 0.9294,  ..., 0.9412, 0.9490, 0.9529],\n",
       "           [0.9373, 0.9373, 0.9333,  ..., 0.9333, 0.9255, 0.9294],\n",
       "           [0.9333, 0.9373, 0.9412,  ..., 0.9333, 0.9255, 0.9294]]]]),\n",
       " tensor([10])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(dl))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = accelerator.prepare(dl)\n",
    "dl = cycle(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "        dim = 64,\n",
    "        dim_mults = (1, 2, 4, 8),\n",
    "        num_classes = 15,\n",
    "        cond_drop_prob = 0.5\n",
    "    ).cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 256,\n",
    "    timesteps = 1000,\n",
    "    sampling_timesteps=250,\n",
    "    loss_type='l2'\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(diffusion.parameters(), lr = 1e-4, betas = (0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(os.path.join('/home/yoos-bii/Desktop/workspace/diffusion_digital_pathology/Checkpoint-diffusion/results_cond_512TO256_GTEX', f'model-{30}.pt'), map_location=accelerator.device)\n",
    "\n",
    "# print(data)\n",
    "\n",
    "model = accelerator.unwrap_model(diffusion)\n",
    "model.load_state_dict(data['model'])\n",
    "\n",
    "opt = opt.load_state_dict(data['opt'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema = EMA(diffusion, beta = (0.9, 0.99), update_every = 0.995)\n",
    "ema = ema.to(accelerator.device)\n",
    "\n",
    "ema.load_state_dict(data['ema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = accelerator.prepare(diffusion, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# model, opt \n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def load(opt, accelerator, milestone, model):\n",
    "    accelerator = accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    data = torch.load(os.path.join('/home/yoos-bii/Desktop/workspace/diffusion_digital_pathology/Checkpoint-diffusion/results_cond_512TO256_GTEX', f'model-{milestone}.pt'), map_location=device)\n",
    "\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model.load_state_dict(data['model'])\n",
    "\n",
    "    step = data['step']\n",
    "    opt.load_state_dict(data['opt'])\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        return ema.load_state_dict(data['ema'])\n",
    "\n",
    "\n",
    "    if 'version' in data:\n",
    "        print(f\"loading from version {data['version']}\")\n",
    "\n",
    "    if exists(accelerator.scaler) and exists(data['scaler']):\n",
    "        accelerator.scaler.load_state_dict(data['scaler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = load(opt, accelerator, '30', model)\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score = FIDEvaluation(\n",
    "    batch_size = 1, \n",
    "    dl=dl, \n",
    "    sampler=ema.ema_model, \n",
    "    # sampler=load_model,\n",
    "    channels=3,\n",
    "    accelerator=accelerator,\n",
    "    # stats_dir='/home/yoos-bii/Desktop/workspace/diffusion_digital_pathology/Checkpoint-diffusion/output_fid',\n",
    "    device=accelerator.device, \n",
    "    num_fid_samples=50000,\n",
    "    inception_block_idx=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Inception features for 50000 samples from the real dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/denoising_diffusion_pytorch/fid_evaluation.py:63\u001b[0m, in \u001b[0;36mFIDEvaluation.load_or_precalc_dataset_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     ckpt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.npz\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm2, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms2 \u001b[39m=\u001b[39m ckpt[\u001b[39m\"\u001b[39m\u001b[39mm2\u001b[39m\u001b[39m\"\u001b[39m], ckpt[\u001b[39m\"\u001b[39m\u001b[39ms2\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/dataset_stats.npz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yoos-bii/Desktop/workspace/diffusion_digital_pathology/02-Diffusion/evaluate.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yoos-bii/Desktop/workspace/diffusion_digital_pathology/02-Diffusion/evaluate.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_fid_score \u001b[39m=\u001b[39m fid_score\u001b[39m.\u001b[39;49mfid_score()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/denoising_diffusion_pytorch/fid_evaluation.py:94\u001b[0m, in \u001b[0;36mFIDEvaluation.fid_score\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39minference_mode()\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfid_score\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_stats_loaded:\n\u001b[0;32m---> 94\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_or_precalc_dataset_stats()\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39meval()\n\u001b[1;32m     96\u001b[0m     batches \u001b[39m=\u001b[39m num_to_groups(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/denoising_diffusion_pytorch/fid_evaluation.py:78\u001b[0m, in \u001b[0;36mFIDEvaluation.load_or_precalc_dataset_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m real_samples \u001b[39m=\u001b[39m real_samples\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     79\u001b[0m real_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_inception_features(real_samples)\n\u001b[1;32m     80\u001b[0m stacked_real_features\u001b[39m.\u001b[39mappend(real_features)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "get_fid_score = fid_score.fid_score()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
